replicas: 1

# Allows you to add any config files in /usr/share/logstash/config/
# such as logstash.yml and log4j2.properties
logstashConfig:
  logstash.yml: |
    xpack.monitoring.elasticsearch.hosts: ["http://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
    http.host: 0.0.0.0

# Allows you to add any pipeline files in /usr/share/logstash/pipeline/
logstashPipeline:
  input_main.conf: |-
    input {
      kafka {
        bootstrap_servers => "${KAFKA_BOOTSTRAP_SERVERS}"
        topics => ["${KAFKA_TOPIC}"]
        group_id => "${KAFKA_TOPIC}"
      }
    }
  filter_main.conf: |-
    filter {
      json {
        source => "message"
        remove_field => ["message"]
      }

      geoip{
        source => "xForwardForIp"
      }
    }

  output_main.conf: |-
    output {
      elasticsearch {
        hosts => ["http://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
        manage_template => false
        index => "${KAFKA_TOPIC}-%{+YYYY.MM.dd}"
        workers => 1
      }
    }

# Extra environment variables to append to this nodeGroup
# This will be appended to the current 'env:' key. You can use any of the kubernetes env
# syntax here
extraEnvs:
  - name: ELASTICSEARCH_HOST
    value: elasticsearch-master-headless.observables.svc
  - name: ELASTICSEARCH_PORT
    value: '9200'
  - name: KAFKA_TOPIC
    value: greymatter
  - name: KAFKA_BOOTSTRAP_SERVERS
    value: kafka-observables-headless.observables.svc:9092
# A list of secrets and their paths to mount inside the pod
secretMounts: []

image: 'docker.elastic.co/logstash/logstash'
imageTag: '7.9.3'
imagePullPolicy: 'IfNotPresent'
imagePullSecrets: []

podAnnotations: {}

# additionals labels
labels: {}

logstashJavaOpts: '-Xmx1g -Xms1g'

resources:
  requests:
    cpu: '100m'
    memory: '1536Mi'
  limits:
    cpu: '1000m'
    memory: '1536Mi'

volumeClaimTemplate:
  accessModes: ['ReadWriteOnce']
  resources:
    requests:
      storage: 1Gi

rbac:
  create: false
  serviceAccountName: ''

podSecurityPolicy:
  create: false
  name: ''
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim

persistence:
  enabled: false
  annotations: {}

extraVolumes:
  ''
  # - name: extras
  #   emptyDir: {}

extraVolumeMounts:
  ''
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

extraContainers:
  ''
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

extraInitContainers:
  ''
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

# This is the PriorityClass settings as defined in
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ''

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
antiAffinityTopologyKey: 'kubernetes.io/hostname'

# Hard means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to soft will do this "best effort"
antiAffinity: 'hard'

# This is the node affinity settings as defined in
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
nodeAffinity: {}

# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: 'Parallel'

httpPort: 9600

updateStrategy: RollingUpdate

# This is the max unavailable setting for the pod disruption budget
# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
maxUnavailable: 1

podSecurityContext:
  # {}
  fsGroup: 1000000000
  runAsUser: 1000000000

securityContext:
  capabilities:
    drop:
      - ALL
  # readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000000000

# How long to wait for logstash to stop gracefully
terminationGracePeriod: 120

livenessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 300
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

readinessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 3

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ''

nodeSelector: {}
tolerations: []

nameOverride: ''
fullnameOverride: ''

lifecycle:
  {}
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

service: {}
#  annotations: {}
#  type: ClusterIP
#  ports:
#    - name: beats
#      port: 5044
#      protocol: TCP
#      targetPort: 5044
#    - name: http
#      port: 8080
#      protocol: TCP
#      targetPort: 8080
